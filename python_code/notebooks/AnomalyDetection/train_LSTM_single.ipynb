{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UHZ5dgtBUukH"
      },
      "outputs": [],
      "source": [
        "# load libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import datetime\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejv9HkvpQlzo",
        "outputId": "fa6ad4b4-3711-4a66-d11b-6c01e4e25751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EdTf6K23VyxH"
      },
      "outputs": [],
      "source": [
        "# change me, have to point to the Meteotrentino drive folder\n",
        "wk =  \"/content/drive/My Drive/10. Meteotrentino/\"\n",
        "\n",
        "# path to the datasets\n",
        "dataset_path = wk+\"DatasetPytorch/24+1_v2/\"\n",
        "\n",
        "# specify sensor type\n",
        "sensor_type = \"pioggia\"\n",
        "\n",
        "# specify name of the sensor type\n",
        "sensor_dataset = \"pioggia1\"\n",
        "\n",
        "# specify code of the weather station\n",
        "sensor = \"T0065\"\n",
        "\n",
        "file_name_test = sensor+\"_test_\"+sensor_dataset+\"_tsf.pt\"\n",
        "file_name_train = sensor+\"_test_\"+sensor_dataset+\"_tsf.pt\"\n",
        "\n",
        "# extrapolate the paths\n",
        "file_test_to_read_path = os.path.join(wk, dataset_path, sensor_type, file_name_test)\n",
        "file_train_to_read_path = os.path.join(wk, dataset_path, sensor_type, file_name_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "axqYKVdiQ46d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "# clone the libraries if not done \n",
        "!git clone https://github.com/eliazonta/AI-Challenge-HIT\n",
        "# append to sys.path to load other modules, needed for the custom dataset\n",
        "sys.path.append(\"/content/AI-Challenge-HIT/python_code/notebooks/PytorchFormatter/\")\n",
        "\n",
        "from custom_datasets_pytorch import CustomDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Foewherb6_GU"
      },
      "outputs": [],
      "source": [
        "from torch import save,load\n",
        "dataset = {}\n",
        "splits = ['train','test']\n",
        "shuffle = {'train':True,'test':False}\n",
        "# here load a pre-existing dataset\n",
        "dataset['train'] = load(file_train_to_read_path)\n",
        "dataset['test'] = load(file_test_to_read_path)\n",
        "# mean and std of train (target is small)\n",
        "mean = dataset['train'].features.mean()\n",
        "std = dataset['train'].features.std()\n",
        "# for training, standardize the dataset\n",
        "dataset['train'].features = (dataset['train'].features - mean)/std\n",
        "dataset['train'].target = (dataset['train'].target - mean)/std\n",
        "dataset['test'].features = (dataset['test'].features - mean)/std\n",
        "dataset['test'].target = (dataset['test'].target - mean)/std\n",
        "\n",
        "# definition of the dataloader\n",
        "b_size = {'train':64,'test':len(dataset['test'])}\n",
        "dataloader = {x: torch.utils.data.DataLoader(dataset=dataset[x],\n",
        "                                            batch_size=b_size[x],\n",
        "                                            shuffle=shuffle[x],\n",
        "                                            collate_fn=lambda x: x,\n",
        "                                            drop_last=True)\n",
        "            for x in splits}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZqHKnrjG4Z2z"
      },
      "outputs": [],
      "source": [
        "# split required if dataset is too fat\n",
        "SPLIT_NUMBER = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RJd1i5QXQ46f",
        "outputId": "f6c325a4-ac2c-406a-b010-637a338caf15"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_20671/1793427177.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# get dome parameters from the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0msequence_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mpred_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Instantiate the model with hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "from collections.abc import Sequence\n",
        "from importlib import reload  # Python 3.4+\n",
        "import torch\n",
        "import sys\n",
        "from torch import nn\n",
        "# clone the libraries if not done \n",
        "#!git clone https://github.com/eliazonta/AI-Challenge-HIT\n",
        "sys.path.append(\"/content/AI-Challenge-HIT/python_code/notebooks/AnomalyDetection/\")\n",
        "import networks\n",
        "LSTMs = reload(networks)\n",
        "from networks import LSTM\n",
        "from tqdm import tqdm\n",
        "\n",
        "# specify the device (\"cuda\" for GPU, \"cpu\" for CPU)\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# get dome parameters from the dataset\n",
        "sequence_length = dataset['train'][0]['features'].size(0)\n",
        "pred_length = dataset['train'][0]['target'].size(0)\n",
        "# Instantiate the model with hyperparameters\n",
        "model = LSTM(input_size=1, output_size=1, hidden_dim=32, num_layers=3, device = device)\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "\n",
        "# We'll also set the model to the device that we defined earlier\n",
        "model = model.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "lr = 1e-5\n",
        "\n",
        "# Define Loss and Optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "n_epochs = 30\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    model.train()\n",
        "    loss_train_batch = []\n",
        "    for data in tqdm(dataloader['train'], leave=True, total=len(dataloader['train'])):\n",
        "      # preparing the input\n",
        "      batch_input_features = data.features.to(device)\n",
        "      target_features = data.target.to(device)\n",
        "      # feed the model and make the prediction\n",
        "      output = model(batch_input_features,pred_length)\n",
        "      output = output.to(device)[:,-pred_length:]\n",
        "      # compute the loss\n",
        "      loss_train = criterion(output, target_features)\n",
        "      # backpropagation and gradients\n",
        "      loss_train.backward() \n",
        "      # Update of the weights accordingly\n",
        "      optimizer.step() \n",
        "      # Append the loss to batch list\n",
        "      loss_train_batch.append(loss_train.item())\n",
        "    \n",
        "    # average on bath train loss\n",
        "    mean_train_loss = np.mean(loss_train_batch)\n",
        "\n",
        "    # test-time\n",
        "    model.eval()\n",
        "    data_test = next(iter(dataloader['test']))\n",
        "    test_input_features = data_test.features.to(device)\n",
        "    test_target_features = data_test.target.to(device)\n",
        "    \n",
        "    # split when it is too fat (out of memory)\n",
        "    test_input_features_splitted = np.array_split(test_input_features, SPLIT_NUMBER)\n",
        "    output_test_list = []\n",
        "    for input_feature_split_subset in test_input_features_splitted:\n",
        "      output_test_list.append(model(input_feature_split_subset,pred_length)[:,-pred_length:])\n",
        "    output_test_flat = [item for item in output_test_list]\n",
        "    output_test = torch.cat(output_test_flat)\n",
        "    # compute loss on test set for validation on unseen data\n",
        "    loss_test = criterion(output_test, test_target_features)\n",
        "    \n",
        "    train_loss.append(mean_train_loss)\n",
        "    test_loss.append(loss_test.item())\n",
        "    print(\"Epoch = \" + str(epoch))\n",
        "    print(\"\\tTraining Loss: {:.4f}\".format(mean_train_loss))\n",
        "    print(\"\\tTest Loss: {:.4f}\".format(loss_test.item()))\n",
        "\n",
        "    # stop criterion on the train loss value\n",
        "    if mean_train_loss < 0.010:\n",
        "        break\n",
        "\n",
        "# plot the training performance (todo: use W&B)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_loss,'o-',label=\"train\")\n",
        "plt.plot(test_loss,'o-',label=\"test\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CXULxyKkohrR"
      },
      "outputs": [],
      "source": [
        "# save model once trained\n",
        "torch.save(model.state_dict(),\n",
        "           os.path.join(wk, \"TrainedModels/\", sensor+\"_\"+sensor_type+\"_model.pt\")\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "db3b0be3bb1f6db1a0432c56ec1c0f7459936b81fd9ed1b468b8c4ba9c3d8298"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
